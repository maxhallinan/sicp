# 1.2

The main concepts are:

- Time and space complexity
- Strategies for reducing time and space complexity

> A procedure is a pattern for the local evolution of a computational process.
> It specifies how each stage of the process is built upon the previous stage.
> We would like to be able to make statements about the overall, or global,
> behavior of a process whose local evolution has been specified by a procedure.
>
> Page 40

The theme of section 1.2 is the relationship between procedures and processes.
A procedure is a function.
A process is the evaluation of a procedure.
The reader should be able to say how a given procedure will be evaluated in
terms of space and time complexity.
How a procedure is defined determines the shape of the resulting process.

## Concepts

### Recursive procedure version recursive process

A recursive _procedure_ and recursive _processes_ are not the same thing.
A procedure is recursive when it is defined in terms of itself.
A process is recursive when the evaluation builds up a chain of sub-expressions 
whose evaluation is deferred until the chain is complete.
Recursive procedures can yield iterative processes, but only if the language
has tail-call optimization.

### Linear recursion

> The substitution model reveals a shape of expansion followed by contraction,
> indicated by the arrow in Figure 13.
> The expansion occurs as the process builds up a chain of _deferred operations_
> (in this case a chain of multiplications). 
> The contraction occurs as the operations are actually performed.
>
> Page 44

A linear recursive process expands a sequence of nested expressions until
recursion terminates.
Then each nested expression is evaluated until the sequence is fully reduced.
Space complexity is O(n) because each expression is pushed onto the stack until 
recursion terminates.
Time complexity is O(n) because the sequence is fully expanded in n steps.

This definition of `factorial` yields a linear recursive process:

```lisp
(define (factorial x) 
  (if (= x 1) 
    x
    (* x (factorial (- x 1)))))
```

```lisp
(factorial 6)
(* 6 (factorial 5))
(* 6 (* 5 (factorial 4)))
(* 6 (* 5 (* 4 (factorial 3))))
(* 6 (* 5 (* 4 (* 3 (factorial 2)))))
(* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 1)))))
(* 6 (* 5 (* 4 (* 3 2))))
(* 6 (* 5 (* 4 6)))
(* 6 (* 5 24))
(* 6 120)
720
```

### Tree recursion

A tree recursive process generates a tree of unevaluated expressions.
The branches of this tree are often redundant, so the same sub-tree is 
computed many times.

### Linear iteration

> In general, an iterative process is one whose state can be summarized by a 
> fixed number of state variables, together with a fixed rule that describes how
> the state variables should be updated as the process moves from state to state
> and an (optional) end test that specifies conditions under which the process
> should terminate.
>
> Page 44

A linear iterative process uses stateful variables instead of the deferred 
evaluation of sub-expressions to build up the computation.
The state is updated at each step of the computation. 
There is less memory overhead because the intermediate steps are evaluated 
immediately.

This definition of `factorial` yields a linear iterative process:

```lisp
(define (factorial x) 
  (define (iter counter product) 
    (if (= counter x) 
      product
      (iter (+ counter 1) (* product counter))))
  (iter 1 x))
```

```lisp
(factorial 6)
(iter (+ 1 1) (* 6 1))
(iter 2 6)
(iter (+ 2 1) (* 6 2))
(iter 3 12)
(iter (+ 3 1) (* 12 3))
(iter 4 36)
(iter (+ 4 1) (* 36 4))
(iter 5 144)
(iter (+ 5 1) (* 144 5))
(iter 6 720)
720
```

### Orders of growth

"Order of growth" is a rough measure of the resources required by a process as
the inputs grow.

`R(n)` is the amount of resources required for a problem of size `n`.

Examples of `n`:

- number of digits accuracy required
- number of internal storage registers used
- number of cpu cycles

`R(n)` grows on the order of `O(f(n))` if there are positive constants `x` and
`y` independent of `n`, such that: `xf(n) <= R(n) <= yf(n)` for any sufficiently
large value of `n`.

- `O(1)`: constant growth, not affected by variance in the size of the input
- `O(n)`: linear growth, proportional to the size of the input
- `O(n^2)`: exponential growth

#### O(log n)

`log(n)` grows linearly with increases in order of magnitude.

```
6 = log(1000000)
5 = log(100000)
4 = log(10000)
3 = log(1000)
2 = log(100)
1 = log(10)
```

## Illustration of concepts

The later parts of section 1.2 use a few different math topics to illustrate 
these ideas.

- Exponentiation
- Greatest common divisor
- Primality

## Questions

### 1.

SICP says that the linear recursive process takes O(n) time: 

> ...with the linear recursive process for computing factorial described in 
> Section 1.2.1 the number of steps grows proportionally to the input n. 
> Thus, the steps required for this process grows as Î˜(n)." 
>
> Page 55

But the linear recursion for `(factorial 6)` takes 12 steps.

```lisp
(* 6 (factorial 5))
(* 6 (* 5 (factorial 4)))
(* 6 (* 5 (* 4 (factorial 3))))
(* 6 (* 5 (* 4 (* 3 (factorial 2)))))
(* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 (* 1 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 1)))))
(* 6 (* 5 (* 4 (* 3 2))))
(* 6 (* 5 (* 4 6)))
(* 6 (* 5 24))
(* 6 120)
(* 6 120)
720
```

Why is the time complexity not O(2n)?

This might be part of the answer:

> Orders of growth provide only a crude description of the behavior of the 
> process.
> For example, a process requiring `n^2` steps and a process requiring `1000n^2`
> steps and a process requiring `3n^2 + 10n + 7` steps all have O(n^2) order
> of growth.
>
> Page 56

But why is this the case?
How do I know to call it O(n^2)?

### 2.

How would I know or determine that the mathematical notation for 
`(define (h n) (A 2 n)) ` is `2^(2^n-1)`?
This is the last piece of exercise 1.10.
My method was to find `h n` for a few small sequential numbers and see if there
was any obvious pattern:

```
(h 1) = 2
(h 2) = 4
(h 3) = 16
(h 4) = 65536
```

I didn't recognize any patterns and then I was at a loss for how to find the
answer.

### 3.

Page 48 - 49 seems to say two things about the recursive Fibonnaci procedure.
First, that it has a time complexity of `O(Fib(n+1))`.
Then it says "Thus, the process uses a number of steps that grows exponentially
with the input."
Which is it, `O(Fib(n+1))` or `O(2^n)`?
How would I be able to identify this myself?

### 4.

I was really stumped on exercise 1.11. 
I spent roughly 10 hours trying different approaches to the iterative piece.
I got close but did not complete it.
I couldn't picture how the intermediate states should be transformed.
How do people know how to transform the intermediate states when designing these
iterative processes?

### 5.

What is an inductive proof?
How do I write an inductive proof?
How do I write an inductive proof for exercise 1.13?

### 6.

> We say that `R(n)` has order of growth `O(f(n))`, written `R(n) = O(f(n))`,
> if there are positive constants `k1` and `k2` independent of `n` such that 
> k1f(n) <= R(n) <= k2f(n) for any sufficiently large value of n.
> (In other words, for large n, the value R(n) is sandwiched between k1f(n) and
> k2f(n)).
>
> Page 55

What does this mean?
And what is its significance?

### 7.

How do I calculate the answer to part b of exercise 1.15?

### 8.

Why does the successive squaring method work for exponentiation?
I see that it works but I can't picture what it's doing.

### 9.

I wasn't able to complete exercise 1.19.
Again, knowing how to transform the intermediates states mystified me.
Do people just think these things up?
Or is there a method for deriving the transformation?

### 10.

Is there a reliable method for identifying the order of growth for an algorithm?
Given a random function, how can I say what the order of growth is?

### 11.

Why is SICP's definition of Fermat's Little Theorem inconsistent with their 
description of the algorithm for Fermat's Little Theorem?

Definition of Fermat's Little Theorem:

> If `n` is a prime number and `a` is any positive integer less than n, then 
> `a` raised to the nth power is congruent to `a` modulo `n`.
>
> (Two numbers are said to be _congruent modulo `n`_ if they both have the same 
> remainder when divided by `n`.
> The remainder of `a` when divided by `n` is also referred to as the remainder
> of `a` modulo `n`, or simply `a` modulo `n`.
>
> Page 67

Based on the above description, I write this code:

```lisp
(define (fermat-test a n) 
  (congruent? (expt a n) (modulo a n) n))

(define (congruent? x y n) 
  (= (modulo x n) 
     (modulo y n)))
```

But then SICP says:

> This leads to the following algorithm for testing primality: Given a number 
> `n`, pick a random number `a < n` and compute the remainder of a^n modulo `n`.
> If the result is not equal to `a`, then `n` is certainly not a prime.
> 
> Page 67

And the book gives this code:

```lisp
(define (fermat-test) 
  (define (try-it)
    (= (expmod a n n) a))
  (try-it (+ 1 (random (- n 1)))))

(define (expmod base exp m) 
  (cond ((= exp 0) 1) 
        ((even? exp) 
          (remainder (expmod base (/ exp 2) m) m)) 
        (else 
          (remainder (* base (expmod base (- exp 1) m)) m))))
```

SICP describes `expmod` as "a procedure that computes the exponential of a 
number modulo another number": `a^b modulo c`.
To me, this doesn't bear any resemblance to Fermat's Little Theorem.
Instead, it looks like this:

What they actually implement looks to me like. 
`n` is prime when:

```
0 < a < n
a = (a^n) modulo n
```

But if I play with this formula in the REPL, there are many times when the
condition holds true when `n` is not prime.

So I don't understand how the code corresponds to the original formulation of 
Fermat's Little Theorem.

**Answer**

I asked this question on StackOverflow [here](https://stackoverflow.com/questions/55950208/what-is-the-correspondence-between-fermats-little-theorem-and-the-sicp-implemen/55950755#55950755).
The answer is that `a modulo n` is superfluous.
`a = a modulo n` as long as `a < n`.
Now I'm curious why the definition would include a check for congruency if it
also stipulates that the `a < n`.

### 12.

Exercise 1.26 shows a procedure that is O(n).
But the procedure is defined using tree recursion.
Tree recursion has exponential growth.
So why is the process O(n)?

> To become experts, we must learn to visualize the processes generated by 
> various types of procedures.
>
> Page 31

> A procedure is a pattern for the _local evolution_ of a computational process.
> 
> Page 31

## Exercise 1.19

**`T`**

```
T = a <- a + b, b <- 
```

Observe that applying `T` over and over again `n` times, starting with 1 and 0,
produces the pair `Fib(n+1)` and `Fib(n)`.

```
T^1 = 1 + 0, 1
    = 1, 1
T^2 = 1 + 1, 1
    = 2, 1
T^3 = 2 + 1, 2
    = 3, 2
T^4 = 3 + 2, 3
    = 5, 3
```

**`Tpq`**


```
p = 0
q = 1
Tpq = a <- bq + aq + ap, b <- bp + aq
```

`Tpq^n` and `T^n` give the same result:

```
Tpq^1 = (0 * 1) + (1 * 1) + (1 * 0), (0 * 1) + (1 * 1)
      = 0 + 1 + 0, 0 + 1
      = 1, 1
Tpq^2 = (1 * 1) + (1 * 1) + (1 * 0), (1 * 0) + (1 * 1)
      = 1 + 1 + 0, 0 + 1
      = 2, 1
Tpq^3 = (1 * 1) + (2 * 1) + (2 * 0), (1 * 0) + (2 * 1)
      = 1 + 2 + 0, 0 + 2
      = 3, 2
Tpq^4 = (2 * 1) + (3 * 1) + (3 * 0), (2 * 0) + (3 * 1)
      = 2 + 3 + 0, 0 + 3
      = 5, 3
Tpq^5 = (3 * 1) + (5 * 1) + (5 * 0), (3 * 0) + (5 * 1)
      = 3 + 5 + 0, 0 + 5
      = 8, 5
Tpq^6 = (5 * 1) + (8 * 1) + (8 * 0), (5 * 0) + (8 * 1)
      = 5 + 8 + 0, 0 + 8
      = 13, 8
```

**Tq'p'**

`Tq'p'` gives the same result in 1 step that `Tqp` does in 2 steps.

`p'` and `q'` are defined in terms of `p` and `q`.

```
a = 1
b = 0
p = 0
q = 1

Tpq = a <- bq + aq + ap, b <- bp + aq

Tp'q'^1 = (0 * q) + (1 * q) + (1 * p), (0 * p) + (1 * q)
        = (0 * 1) + (1 * 1) + (1 * 1), (0 * 1) + (1 * 1)
        = 0 + 1 + 1, 0 + 1
        = 2, 1

Tp'q'^2 = a <- bq + aq + ap, b <- bp + aq
        = (0 * 3) + (1 * 3) + (1 * 2), (0 * 2) + (1 * 3)
        = 0 + 3 + 2, 0 + 3
        = 5, 3
```

It seems like the `p` increases by `Fib(n)` and `q` increases by `Fib(n+1)`.
But when I try this, the formula doesn't work.

The closest I've gotten is:

```lisp
(define (fib-iter a b x y count)
  (cond ((= count 0) b)
        ((even? count)
         (fib-iter a
                   b
                   y
                   (fib-iter 1 0 0 1 (square y))
                   (/ count 2)))
        (else (fib-iter (+ (* b y) (* a y) (* a x))
                        (+ (* b x) (* a y))
                        x
                        y
                        (- count 1)))))
```

This calculates the first 3 places in the sequence correctly.

This one gets some of them right:

```scheme
(define (fib-iter a b x y count)
  (cond ((= count 0) b)
        ((even? count)
         (fib-iter a
                   b
                   (fib-iter 1 0 0 1 (square y))
                   (+ x y)
                   (/ count 2)))
        (else (fib-iter (+ (* b y) (* a y) (* a x))
                        (+ (* b x) (* a y))
                        x
                        y
                        (- count 1)))))

```

I could also see `(square (fib-iter 1 0 0 1 y))` making sense.

This gets some of them right:

```scheme
(define (fib-iter a b x y count)
  (cond ((= count 0) b)
        ((even? count)
         (fib-iter a
                   b
                   (square (fib-iter 1 0 0 1 y))
                   (+ x 1)
                   (/ count 2)))
        (else (fib-iter (+ (* b y) (* a y) (* a x))
                        (+ (* b x) (* a y))
                        x
                        y
                        (- count 1)))))
```

## Primality

### Method 1: Search for divisors

If `n` is not prime, it must have a divisor less than or equal to the square 
root of `n`.
This method tries each number between 1 and the square root of `n` to see if 
that number is a divisor of `n`.
Thus, the time complexity is O(square-root(n)).

```scheme
(define (prime? n) 
  (= n (smallest-divisor n)))

(define (smallest-divisor n) 
  (find-divisor n 2))

(define (find-divisor n test-divisor) 
  (cond ((> (square test-divisor) n) n)
        ((divides? test-divisor n) test-divisor)
        (else (find-divisor n (+ test-divisor 1)))))

(define (divides? a b) 
  (= (remainder b a) 0))
```

### Method 2: The Fermat test

#### Fermat's Little Theorem

> If `n` is a prime number and `a` is any positive integer less than `n`, then
> `a` raised to the nth power is congruent to `a` modulo `n`.

Two numbers are "congruent modulo n" if they both have the same remainder 
divided by `n`.

Fermat test:

- Given a number `n` 
- Pick a random number `a < n` 
- Compute the remainder `a^n modulo n`
- If `(a^n modulo n) != a`, n is not prime
- If it is `a`, pick another random number and test with the same method
- Confidence that `n` is prime grows with the number of tests

Compute the exponential of a number modulo another number:

```scheme
; run the fermat test a given number of times
(define (fast-prime? n times) 
  (cond ((= times 0) true) 
        ((fermat-test n) (fast-prime? n (- times 1))) 
        (else false)))

(define (fermat-test n) 
  (define (try-it a) 
    (= (expmod a n n) a)) 
  (try-it (+ 1 (random (- n 1)))))

(define (expmod base exp m) 
  (cond ((= exp 0) 1) 
        ((even? exp) 
          (remainder (square (expmod base (/ exp 2) m)) m)) 
        (else 
          (remainder (* base (expmod base (- exp 1))) m))))
```

#### Fermat's Little Theorem 2

`n` is prime when:

```
0 < a < n
a^n-1 â‰¡ 1 (mod n)
```

This notation means that `a^n-1 mod n = 1 mod n`.

### Miller-Rabin Test

Pick a random number `a`, where `0 < a < n`.

```
0 < a < n
a^n-1 â‰¡ 
```
