# 1.2

The main concepts are:

- Time and space complexity
- Strategies for reducing time and space complexity

> A procedure is a pattern for the local evolution of a computational process.
> It specifies how each stage of the process is built upon the previous stage.
> We would like to be able to make statements about the overall, or global,
> behavior of a process whose local evolution has been specified by a procedure.
>
> Page 40

The theme of section 1.2 is the relationship between procedures and processes.
A procedure is a function.
A process is the evaluation of a procedure.
The reader should be able to say how a given procedure will be evaluated in
terms of space and time complexity.
How a procedure is defined determines the shape of the resulting process.

## Concepts

### Recursive procedure version recursive process

A recursive _procedure_ and recursive _processes_ are not the same thing.
A procedure is recursive when it is defined in terms of itself.
A process is recursive when the evaluation builds up a chain of sub-expressions 
whose evaluation is deferred until the chain is complete.
Recursive procedures can yield iterative processes, but only if the language
has tail-call optimization.

### Linear recursion

> The substitution model reveals a shape of expansion followed by contraction,
> indicated by the arrow in Figure 13.
> The expansion occurs as the process builds up a chain of _deferred operations_
> (in this case a chain of multiplications). 
> The contraction occurs as the operations are actually performed.
>
> Page 44

A linear recursive process expands a sequence of nested expressions until
recursion terminates.
Then each nested expression is evaluated until the sequence is fully reduced.
Space complexity is O(n) because each expression is pushed onto the stack until 
recursion terminates.
Time complexity is O(n) because the sequence is fully expanded in n steps.

This definition of `factorial` yields a linear recursive process:

```lisp
(define (factorial x) 
  (if (= x 1) 
    x
    (* x (factorial (- x 1)))))
```

```lisp
(factorial 6)
(* 6 (factorial 5))
(* 6 (* 5 (factorial 4)))
(* 6 (* 5 (* 4 (factorial 3))))
(* 6 (* 5 (* 4 (* 3 (factorial 2)))))
(* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 1)))))
(* 6 (* 5 (* 4 (* 3 2))))
(* 6 (* 5 (* 4 6)))
(* 6 (* 5 24))
(* 6 120)
720
```

### Tree recursion

A tree recursive process generates a tree of unevaluated expressions.
The branches of this tree are often redundant, so the same sub-tree is 
computed many times.

### Linear iteration

> In general, an iterative process is one whose state can be summarized by a 
> fixed number of state variables, together with a fixed rule that describes how
> the state variables should be updated as the process moves from state to state
> and an (optional) end test that specifies conditions under which the process
> should terminate.
>
> Page 44

A linear iterative process uses stateful variables instead of the deferred 
evaluation of sub-expressions to build up the computation.
The state is updated at each step of the computation. 
There is less memory overhead because the intermediate steps are evaluated 
immediately.

This definition of `factorial` yields a linear iterative process:

```lisp
(define (factorial x) 
  (define (iter counter product) 
    (if (= counter x) 
      product
      (iter (+ counter 1) (* product counter))))
  (iter 1 x))
```

```lisp
(factorial 6)
(iter (+ 1 1) (* 6 1))
(iter 2 6)
(iter (+ 2 1) (* 6 2))
(iter 3 12)
(iter (+ 3 1) (* 12 3))
(iter 4 36)
(iter (+ 4 1) (* 36 4))
(iter 5 144)
(iter (+ 5 1) (* 144 5))
(iter 6 720)
720
```

### Orders of growth

"Order of growth" is a rough measure of the resources required by a process as
the inputs grow.

`R(n)` is the amount of resources required for a problem of size `n`.

Examples of `n`:

- number of digits accuracy required
- number of internal storage registers used
- number of cpu cycles

`R(n)` grows on the order of `O(f(n))` if there are positive constants `x` and
`y` independent of `n`, such that: `xf(n) <= R(n) <= yf(n)` for any sufficiently
large value of `n`.

- `O(1)`: constant growth, not affected by variance in the size of the input
- `O(n)`: linear growth, proportional to the size of the input
- `O(n^2)`: exponential growth

#### O(log n)

`log(n)` grows linearly with increases in order of magnitude.

```
6 = log(1000000)
5 = log(100000)
4 = log(10000)
3 = log(1000)
2 = log(100)
1 = log(10)
```

#### Estimating order of growth

Based on [this](https://stackoverflow.com/a/4852666/3125444) Stackoverflow 
answer.

There is no mechanical procedure for calculating Big-Oh.

Example, this function that sums an array of numbers:

```javascript
function sum(xs) {
  let result = 0;                       // 1

  for (let i = 0; i < xs.length; i++) { // 2
    result += xs[i];                    // 3
  }

  return result;                        // 4
}
```

First, define a function `f(N)` that represents the steps taken by the function,
where `N` is the size of the input.

Steps 1 and 4 are constant because these steps do not grow or decrease with the
size of the input.
Any number of steps that are unaffected by the size of the input are constant.
Use `C` to represent those constants.

```
f(N) = C + ??? + C
```

The statement within the loop is a constant `C`.
But the iterations of the loop _are_ proportional to the size of the input.
The loop iterates as many times as there are elements.
So the loop does a constant step n times, or `N * C`.
The variable initialization and incrementing in the head of the for loop can be
ignored because they don't have a significant impact on the outcome.

```
f(N) = C + N * C + C
```

Somehow the stackoverflow answer goes from the above to:

```
f(N) = 2 * C * N ^ 0 + 1 * C * N ^ 1
```

Asymptotic analysis of a function:

1. Take away all constants `C`.
1. From `f()` get the polynomium in its standard form.
1. Divide the terms of the polynomium and sort them by rate of growth.
1. Keep the one that grows bigger when `N` approaches `infinity`.

The basic approach seems to be to assign a value to each element of a procedure.
Find the term that has the greatest value.
Drop all the other values.
Drop coefficients and constants from the remaining term.

## Illustration of concepts

The later parts of section 1.2 use a few different math topics to illustrate 
these ideas.

- Exponentiation
- Greatest common divisor
- Primality

## Questions

### 1.

SICP says that the linear recursive process takes O(n) time: 

> ...with the linear recursive process for computing factorial described in 
> Section 1.2.1 the number of steps grows proportionally to the input n. 
> Thus, the steps required for this process grows as Θ(n)." 
>
> Page 55

But the linear recursion for `(factorial 6)` takes 12 steps.

```lisp
(* 6 (factorial 5))
(* 6 (* 5 (factorial 4)))
(* 6 (* 5 (* 4 (factorial 3))))
(* 6 (* 5 (* 4 (* 3 (factorial 2)))))
(* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 (* 1 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 1)))))
(* 6 (* 5 (* 4 (* 3 2))))
(* 6 (* 5 (* 4 6)))
(* 6 (* 5 24))
(* 6 120)
(* 6 120)
720
```

Why is the time complexity not O(2n - 1)?

**Answer**

> Orders of growth provide only a crude description of the behavior of the 
> process.
> For example, a process requiring `n^2` steps and a process requiring `1000n^2`
> steps and a process requiring `3n^2 + 10n + 7` steps all have O(n^2) order
> of growth.
>
> Page 56

Constants and coefficients are ignored when determining the Big-O of a function.
Big-O is not an exact measurement of growth but an approximation of the worst 
case.
This kind of analysis is called asymptotic analysis.
Asymptotic analysis is about equivalence rather than exactitude.
`2n - 1` is "asymptotically equivalent" to `n` as `n` approaches infinity.
This means that the difference between `2n - 1` and `n` is relatively 
insignificant when `n` is a very large number.

> Because Big-O only deals in approximation, we drop the 2 entirely, because the
> difference between 2n and n isn't fundamentally different.
> The growth is still linear, it's just a faster growing linear function.
>
> ["Big-O is easy to calculate, if you know how"](https://justin.abrah.ms/computer-science/how-to-calculate-big-o.html)

Coefficients and constants are dropped because they change the _rate_ of growth,
not the _type_ of growth.

### 2.

How would I know or determine that the mathematical notation for 
`(define (h n) (A 2 n)) ` is `2^(2^n-1)`?
This is the last piece of exercise 1.10.
My method was to find `h n` for a few small sequential numbers and see if there
was any obvious pattern:

```
(h 1) = 2
(h 2) = 4
(h 3) = 16
(h 4) = 65536
```

I didn't recognize any patterns and then I was at a loss for how to find the
answer.

**Answer**

I asked this question on StackOverflow [here](https://stackoverflow.com/questions/55955688/find-the-concise-mathematical-definition-of-a-function-in-sicp/55963885#55963885).
The answers said to use the substitution method.

```racket
(define (A x y)
  (cond
    ((= y 0) 0)
    ((= x 0) (* 2 y))
    ((= y 1) 2)
    (else (A (- x 1) (A x (- y 1))))))
(define (h n) (A 2 n))
```

```racket
(h 2)
(A 2 2)
(A (- 2 1) (A 2 (- 2 1)))
(A 1 (A 2 1))
(A 1 2)
(A (- 1 1) (A x (- 2 1)))
(A 0 (A 1 1))
(A 0 2)
(* 2 2)
4
```

```racket
(h 3)
(A 2 3)
(A (- 2 1) (A 2 (- 3 1)))
(A 1 (A 2 2))
(A 1 (A (- 2 1) (A 2 (- 2 1))))
(A 1 (A 1 (A 2 1)))
(A 1 (A 1 2))
(A 1 (A (- 1 1) (A 1 (- 2 1))))
(A 1 (A 0 (A 1 1)))
(A 1 (A 0 2))
(A 1 4)
(A (- 1 1) (A 1 (- 4 1)))
(A 0 (A 1 3))
(A 0 (A (- 1 1) (A 1 (- 3 1))))
(A 0 (A 0 (A 1 2)))
(A 0 (A 0 (A (- 1 1) (A 1 (- 2 1)))))
(A 0 (A 0 (A 0 (A 1 1))))
(A 0 (A 0 (A 0 2)))
(A 0 (A 0 4))
(A 0 8)
16
```

But I'm not able to abstract the above to a mathematical definition.
So it's not exactly stepping through the evaluation of the procedure the yields
the answer.

The StackOverflow answer describes this way of reasoning:

Start by understanding that `(A 0 n)` is equivalent to 2n.
This is easy to see by looking at the code.
For all n, `(A 0 n)` evaluates to `(* 2 n)`.

```racket
(A 0 n)

(cond
  ((= y 0) 0)
  ((= x 0) (* 2 y)) ; <- 2n
  ((= y 1) 2)
  (else (A (- x 1) (A x (- y 1))))))
```

Then the answer says that `(A 1 n)` is: `(A 0 (A 1 (- n 1)))`.
This abstracts to: `(* 2 (A 1 (- n 1)))`.
Now understand that `(A x 1) = 2` for all `x`.
Thus, `(A 1 n)` builds up a chain of `(* 2 (* 2 (* 2 ...2)))` that is as 
long as `n`.
Thus, `(A 1 n) = 2^n`.

A similar reasoning for `(A 2 n)` looks like this:

`(A 2 n)` is `(A 1 (A 2 (- n 1)))`.
We already now that `(A 1 n)` is `2^n`.
Replace `(A 1 (A 2 (- n 1)))` with `(expt (A 2 (- n 1)) 2)`.
If we expand this a second step, we get `(expt (expt (A 2 (- n 1)) 2) 2)`.
So `(A 2 n)` is `2^(2^(2^...))` to the depth of `n`.

### 3.

Page 48 - 49 seems to say two things about the recursive Fibonnaci procedure.
First, that it has a time complexity of `O(Fib(n+1))`.
Then it says "Thus, the process uses a number of steps that grows exponentially
with the input."
Which is it, `O(Fib(n+1))` or `O(2^n)`?
How would I be able to identify this myself?

**Answer**

The tree recursive process:

```racket
(define (fib n) 
  (cond ((= n 0) 0) 
        ((= n 1) 1) 
        (else (+ (fib (- n 1)) 
                 (fib (- n 2))))))
```

SICP says:

> the number of times the procedure will compute (fib 1) or (fib 0) (the number 
> of leaves in the above tree, in general) is precisely Fib(n + 1)

I can show this using the substitution method:

```racket
; 1 time = fib(0 + 1)
(fib 0)

; 1 time = (fib 1 + 1)
(fib 1)

; 2 times = (fib 2 + 1)
(fib 2)
(+ (fib 1) (fib 0))

; 3 times = (fib 3 + 1)
(fib 3)
(+ (fib 2) (fib 1))
(+ (+ (fib 1) (fib 0)) (fib 1))

; 4 times = (fib 4 + 1)
(fib 4)
(+ (fib 3) (fib 2))
(+ (+ (fib 2) (fib 1)) (fib 2))
(+ (+ (+ (fib 1) (fib 0)) (fib 1)) (fib 2))
(+ (+ (+ (fib 1) (fib 0)) (fib 1)) (+ (fib 1) (fib 0)))
```

But I'm not sure how to show or fit this process to exponential growth.
For example, 

| fib | steps |
|-----|-------|
| (fib 3) | ~2 |
| (fib 4) | ~4 |
| (fib 5) | ~8 |
| (fib 6) | ~32 |

```racket
; ~2 steps
(fib 3)
(+ (fib 2) (fib 1))
(+ (+ (fib 1) (fib 0)) (fib 1))

; ~4 steps
(fib 4)
(+ (fib 3) (fib 2))
(+ (+ (fib 2) (fib 1)) (fib 2))
(+ (+ (+ (fib 1) (fib 0)) (fib 1)) (fib 2))
(+ (+ (+ (fib 1) (fib 0)) (fib 1)) (+ (fib 1) (fib 0)))

#|
~8 steps
|#
(fib 5)
(+ (fib 4) (fib 3))
(+ (+ (fib 3) (fib 2)) (fib 3))
(+ (+ (+ (fib 2) (fib 1)) (fib 2)) (fib 3))
(+ (+ (+ (+ (fib 1) (fib 0)) (fib 1)) (fib 2)) (fib 3))
(+ (+ (+ (+ (fib 1) (fib 0)) (fib 1)) (+ (fib 1) (fib 0))) (fib 3))
(+ (+ (+ (+ (fib 1) (fib 0)) (fib 1)) (+ (fib 1) (fib 0))) (+ (fib 2) (fib 1)))
(+ (+ (+ (+ (fib 1) (fib 0)) (fib 1)) (+ (fib 1) (fib 0))) (+ (+ (fib 1) (fib 0)) (fib 1)))

; ~32 steps
(fib 6)
(+ (fib 5) (fib 4))
```

I see that there's an exponential flavor to this but I can't show exactly that
it's exponential.
Is it because the order of maginitude for n and O(n) don't increase at the same
time?
Maybe it's because the actual formula is off by a constant factor? 
For example, if `(fib 6)` has 32 steps, that's 2^(n-1).
If `(fib 5)` has 8 steps, then that's 2^(n-2).
In both cases, the constant factors are dropped and the result is O(2^n).
I'm not sure if this is right or not.

### 4.

I was really stumped on exercise 1.11. 
I spent roughly 10 hours trying different approaches to the iterative piece.
I got close but did not complete it.
I couldn't picture how the intermediate states should be transformed.
How do people know how to transform the intermediate states when designing these
iterative processes?
It seemed more like a math problem than a programming problem.

### 5.

What is an inductive proof?
How do I write an inductive proof?
How do I write an inductive proof for exercise 1.13?

### 6.

> We say that `R(n)` has order of growth `Θ(f(n))`, written `R(n) = Θ(f(n))`,
> if there are positive constants `k1` and `k2` independent of `n` such that 
> k1f(n) <= R(n) <= k2f(n) for any sufficiently large value of n.
> (In other words, for large n, the value R(n) is sandwiched between k1f(n) and
> k2f(n)).
>
> Page 55

What does this mean?
And what is its significance?

Maybe k1f(n) ≤ R(n) means that there is some observable growth? 
And maybe R(n) ≤ k2f(n) means that f(n) is the upper limit of the growth? 
But if R(n) = Θ(f(n)) and k1 is a positive constant, when would k1f(n) ever be 
less than R(n)? 
It seems like the condition holds only when k1 is 1.

**Answer**

I asked this question on StackOverflow [here](https://stackoverflow.com/questions/55983265/what-are-the-constants-referred-to-in-sicps-definition).

This is a definition of Big-Theta, not Big-O.
I've been thinking that Theta and O were the same, but they are not.
Big-Theta is a "tighter" approximation of growth than Big-O.

#### Big-O

[This](https://stackoverflow.com/questions/25657108/constants-in-the-formal-definition-of-big-o)
StackOverflow post addresses the meaning of the constants in the formal 
definition of Big-O.

The questions gives this formal definition for Big-O:

> f(n) = O(g(n)) when there exists a constant c such that f(n) is always 
> <= c*g(n) for a some value of n > N

And the question is why multiply g(n) by c?
It seems like you could cheat by making c a huge value and almost guarantee 
that it's greater than f(n).
But this isn't true.
You can't guarantee that c is a large enough value to make the condition true
unless you know the value of n.
The condition only holds true for all values of c if g(n) is a "constant factor"
away from f(n).

This means that g(n) doesn't have to be an exactly accurate description of the
growth rate for f(n).
It just means that it's reasonably close.
And this also means that the same order of growth can be used to describe a set
of functions, not just one function.
The actual growth rates are in proximity to their Big-O growth rate, as 
described by that condition.

For example, f(n) = n^3 and we say that g = n.
The condition f(n) <= c*g(n) tells us whether or not g(n) if the correct order
of growth for f(n).
In this case, it's not.
And we know that because not all values of c make g(n) >= f(n).
You can't guarantee that any value of c makes c*g(n) >= f(n).
c would have to be n^2, at which point the distance between f(n) and g(n) is
not a constant factor.

So what this formal definition says is that f(n) has an order of growth g(n) if
g(n) is reasonably close to f(n), a "constant factor" close.

[This](https://stackoverflow.com/questions/2754718/big-oh-notation-formal-definition)
is another StackOverflow post on the topic.

The question uses essentially the same formal definition of Big-O:

> A function f(n) is of order at most g(n) - that is, f(n) = O(g(n)) - if a 
> positive real number c and positive integer N exist such that f(n) <= c g(n) 
> for all n >= N.
> That is, c g(n) is an upper bound on f(n) when n is sufficiently large.

and gives this example:

> In segment 9.4, we said that an algorithm that uses 5n + 3 operations is O(n).
> We now can show that 5n + 3 = O(n) by using the formal definition of Big-O.
>
> When n >= 3, 5n + 3 <= 5n + n = 6n. Thus, if we let f(n) = 5n + 3, g(n) = n,
> c = 6, N = 3, we have shown that f(n) <= 6 g(n) for n >= 3, or 5n + 3 = O(n).
> That is, if an algorithm requires time directly proportional to 5n + 3, it is 
> O(n).

and a second example:

> Let's show that 4n^2 + 50n - 10 = O(n^2).
> It is easy to see that: 4n^2 + 50n - 10 <= 4n^2 + 50n for any n.
> Since 50n <= 50n^2 for n = 50, 4n^2 + 50 - 10 <= 4n^2 + 50n^2 = 54n^2 for n >= 50.
> Thus, with c = 54 and N = 50, we have shown that 4n^2 + 50n - 10 = O(n^2).

But I don't understand how they get the intermediate steps in these examples.
In the first example, they say "When n >= 3, 5n + 3 <= 5n + n = 6n".
But where did 5n + n come from?

In the second example, where did 
4n^2 + 50 - 10 <= 4n^2 + 50n^2 = 54n^2 for n >= 50 come from?

### 7.

How do I calculate the answer to part b of exercise 1.15?

**Answer**

I used the substitution method similar to the answer to question 2 above.
Basically, the only non-constant operation in `sine` is the invocation of `p`.
So, I counted how many times `p` is applied for 1, 10, 1000, and 10000.
That was 5, 7, 10, and 13.
So, the number of steps increases linearly with the order of magnitude of `n`,
making it `O(log n)`.

### 8.

Why does the successive squaring method work for exponentiation?
I see that it works but I can't picture what it's doing.

**Answer**

The principle here is the equivalency between 2 * 2 * 2 * 2 and 2^2 * 2^2.
An exponentiation procedure that's O(n) would take the first approach.
But the successive squaring approach takes many less steps:

|-------------------|-------|
| fast-expt(n)      | steps |
|-------------------|-------|
| fast-expt(10)     | 5     |
| fast-expt(100)    | 9     |
| fast-expt(1000)   | 15    |
|-------------------|-------|

So `fast-expt` is O(log n) because as the order magnitude for n increases, the
steps are still within a constant factor of difference.

```racket
(define (fast-expt b n) 
  (cond ((= n 0) 1) 
        ((even? n) (square (fast-expt b (/ n 2)))) 
        (else (* b (fast-expt b (- n 1))))))
```

Here is the substitution method:

```racket
(fast-expt 2 1)
(* 2 (fast-expt 2 0))

(fast-expt 2 2)
(square (fast-expt 2 1))
(square (* 2 (fast-expt 2 0)))
(square (* 2 1))
(square 2)
4

(fast-expt 2 3)
(* 2 (fast-expt 2 2))
(* 2 (square (fast-expt 2 1)))
(* 2 (square (* 2 (fast-expt 2 0))))
(* 2 (square (* 2 1)))
(* 2 (square 2))
(* 2 4)
8

(fast-expt 2 4)
(square (fast-expt 2 2))
(square (square (fast-expt 2 1)))
(square (square (* 2 (fast-expt 2 0))))
(square (square (* 2 1)))
(square (square 2))
(square 4)
16

(fast-expt 2 5)
(* 2 (fast-expt 2 4))
(* 2 (square (fast-expt 2 2)))
(* 2 (square (square (fast-expt 2 1))))
(* 2 (square (square (* 2 (fast-expt 2 0)))))
(* 2 (square (square (* 2 1))))
(* 2 (square (square 2)))
(* 2 (square 4))
(* 2 16)
32

(fast-expt 2 6)
(square (fast-expt 2 3))
(square (* 2 (fast-expt 2 2)))
(square (* 2 (square (fast-expt 2 1))))
(square (* 2 (square (* 2 (fast-expt 2 0)))))
(square (* 2 (square (* 2 1))))
(square (* 2 (square 2)))
(square (* 2 4))
(square 8)
64

(fast-expt 2 10)
(square (fast-expt 2 5))
(square (* 2 (fast-expt 2 4)))
(square (* 2 (square (fast-expt 2 2))))
(square (* 2 (square (square (fast-expt 2 1)))))
(square (* 2 (square (square (* 2 (fast-expt 2 0))))))
; 5 steps

(fast-expt 2 100)
(square (fast-expt 2 50))
(square (square (fast-expt 2 25)))
(square (square (* 2 (fast-expt 2 24))))
(square (square (* 2 (square (fast-expt 2 12)))))
(square (square (* 2 (square (square (fast-expt 2 6))))))
(square (square (* 2 (square (square (square (fast-expt 2 3)))))))
(square (square (* 2 (square (square (square (* 2 (fast-expt 2 2))))))))
(square (square (* 2 (square (square (square (* 2 (square (fast-expt 2 1)))))))))
(square (square (* 2 (square (square (square (* 2 (square (* 2 (fast-expt 2 0))))))))))
; 9 steps

(fast-expt 2 1000)
(square (fast-expt 2 500))
(square (square (fast-expt 2 250)))
(square (square (square (fast-expt 2 125))))
(square (square (square (* 2 (fast-expt 2 124)))))
(square (square (square (* 2 (square (fast-expt 2 62))))))
(square (square (square (* 2 (square (square (fast-expt 2 31)))))))
(square (square (square (* 2 (square (square (* 2 (fast-expt 2 30))))))))
(square (square (square (* 2 (square (square (* 2 (square (fast-expt 2 15)))))))))
(square (square (square (* 2 (square (square (* 2 (square (* 2 (fast-expt 2 14))))))))))
(square (square (square (* 2 (square (square (* 2 (square (* 2 (square (fast-expt 2 7)))))))))))
(square (square (square (* 2 (square (square (* 2 (square (* 2 (square (* 2 (fast-expt 2 6))))))))))))
(square (square (square (* 2 (square (square (* 2 (square (* 2 (square (* 2 (square (fast-expt 2 3)))))))))))))
(square (square (square (* 2 (square (square (* 2 (square (* 2 (square (* 2 (square (* 2 fast-expt 2 2)))))))))))))
(square (square (square (* 2 (square (square (* 2 (square (* 2 (square (* 2 (square (* 2 (square (fast-expt 2 1)))))))))))))))
(square (square (square (* 2 (square (square (* 2 (square (* 2 (square (* 2 (square (* 2 (square (* 2 (fast-expt 2 0))))))))))))))))
; 15 steps
```

### 9.

I wasn't able to complete exercise 1.19.
Again, knowing how to transform the intermediates states mystified me.
Do people just think these things up?
Or is there a method for deriving the transformation?

### 10.

Is there a reliable method for identifying the order of growth for an algorithm?
Given a random function, how can I say what the order of growth is?

### 11.

Why is SICP's definition of Fermat's Little Theorem inconsistent with their 
description of the algorithm for Fermat's Little Theorem?

Definition of Fermat's Little Theorem:

> If `n` is a prime number and `a` is any positive integer less than n, then 
> `a` raised to the nth power is congruent to `a` modulo `n`.
>
> (Two numbers are said to be _congruent modulo `n`_ if they both have the same 
> remainder when divided by `n`.
> The remainder of `a` when divided by `n` is also referred to as the remainder
> of `a` modulo `n`, or simply `a` modulo `n`.
>
> Page 67

Based on the above description, I write this code:

```lisp
(define (fermat-test a n) 
  (congruent? (expt a n) (modulo a n) n))

(define (congruent? x y n) 
  (= (modulo x n) 
     (modulo y n)))
```

But then SICP says:

> This leads to the following algorithm for testing primality: Given a number 
> `n`, pick a random number `a < n` and compute the remainder of a^n modulo `n`.
> If the result is not equal to `a`, then `n` is certainly not a prime.
> 
> Page 67

And the book gives this code:

```lisp
(define (fermat-test) 
  (define (try-it)
    (= (expmod a n n) a))
  (try-it (+ 1 (random (- n 1)))))

(define (expmod base exp m) 
  (cond ((= exp 0) 1) 
        ((even? exp) 
          (remainder (expmod base (/ exp 2) m) m)) 
        (else 
          (remainder (* base (expmod base (- exp 1) m)) m))))
```

SICP describes `expmod` as "a procedure that computes the exponential of a 
number modulo another number": `a^b modulo c`.
To me, this doesn't bear any resemblance to Fermat's Little Theorem.
Instead, it looks like this:

What they actually implement looks to me like. 
`n` is prime when:

```
0 < a < n
a = (a^n) modulo n
```

But if I play with this formula in the REPL, there are many times when the
condition holds true when `n` is not prime.

So I don't understand how the code corresponds to the original formulation of 
Fermat's Little Theorem.

**Answer**

I asked this question on StackOverflow [here](https://stackoverflow.com/questions/55950208/what-is-the-correspondence-between-fermats-little-theorem-and-the-sicp-implemen/55950755#55950755).
The answer is that `a modulo n` is superfluous.
`a = a modulo n` as long as `a < n`.
Now I'm curious why the definition would include a check for congruency if it
also stipulates that the `a < n`.

### 12.

Exercise 1.26 shows a procedure that is O(n).
But the procedure is defined using tree recursion.
Tree recursion has exponential growth.
So why is the process O(n)?

**Answer**

Using the substitution method revealed that the procedure took roughly 2n steps,
where n is the `expt` argument.
2n ~ O(n).

## Exercise 1.19

**`T`**

```
T = a <- a + b, b <- 
```

Observe that applying `T` over and over again `n` times, starting with 1 and 0,
produces the pair `Fib(n+1)` and `Fib(n)`.

```
T^1 = 1 + 0, 1
    = 1, 1
T^2 = 1 + 1, 1
    = 2, 1
T^3 = 2 + 1, 2
    = 3, 2
T^4 = 3 + 2, 3
    = 5, 3
```

**`Tpq`**


```
p = 0
q = 1
Tpq = a <- bq + aq + ap, b <- bp + aq
```

`Tpq^n` and `T^n` give the same result:

```
Tpq^1 = (0 * 1) + (1 * 1) + (1 * 0), (0 * 1) + (1 * 1)
      = 0 + 1 + 0, 0 + 1
      = 1, 1
Tpq^2 = (1 * 1) + (1 * 1) + (1 * 0), (1 * 0) + (1 * 1)
      = 1 + 1 + 0, 0 + 1
      = 2, 1
Tpq^3 = (1 * 1) + (2 * 1) + (2 * 0), (1 * 0) + (2 * 1)
      = 1 + 2 + 0, 0 + 2
      = 3, 2
Tpq^4 = (2 * 1) + (3 * 1) + (3 * 0), (2 * 0) + (3 * 1)
      = 2 + 3 + 0, 0 + 3
      = 5, 3
Tpq^5 = (3 * 1) + (5 * 1) + (5 * 0), (3 * 0) + (5 * 1)
      = 3 + 5 + 0, 0 + 5
      = 8, 5
Tpq^6 = (5 * 1) + (8 * 1) + (8 * 0), (5 * 0) + (8 * 1)
      = 5 + 8 + 0, 0 + 8
      = 13, 8
```

**Tq'p'**

`Tq'p'` gives the same result in 1 step that `Tqp` does in 2 steps.

`p'` and `q'` are defined in terms of `p` and `q`.

```
a = 1
b = 0
p = 0
q = 1

Tpq = a <- bq + aq + ap, b <- bp + aq

Tp'q'^1 = (0 * q) + (1 * q) + (1 * p), (0 * p) + (1 * q)
        = (0 * 1) + (1 * 1) + (1 * 1), (0 * 1) + (1 * 1)
        = 0 + 1 + 1, 0 + 1
        = 2, 1

Tp'q'^2 = a <- bq + aq + ap, b <- bp + aq
        = (0 * 3) + (1 * 3) + (1 * 2), (0 * 2) + (1 * 3)
        = 0 + 3 + 2, 0 + 3
        = 5, 3
```

It seems like the `p` increases by `Fib(n)` and `q` increases by `Fib(n+1)`.
But when I try this, the formula doesn't work.

The closest I've gotten is:

```lisp
(define (fib-iter a b x y count)
  (cond ((= count 0) b)
        ((even? count)
         (fib-iter a
                   b
                   y
                   (fib-iter 1 0 0 1 (square y))
                   (/ count 2)))
        (else (fib-iter (+ (* b y) (* a y) (* a x))
                        (+ (* b x) (* a y))
                        x
                        y
                        (- count 1)))))
```

This calculates the first 3 places in the sequence correctly.

This one gets some of them right:

```scheme
(define (fib-iter a b x y count)
  (cond ((= count 0) b)
        ((even? count)
         (fib-iter a
                   b
                   (fib-iter 1 0 0 1 (square y))
                   (+ x y)
                   (/ count 2)))
        (else (fib-iter (+ (* b y) (* a y) (* a x))
                        (+ (* b x) (* a y))
                        x
                        y
                        (- count 1)))))

```

I could also see `(square (fib-iter 1 0 0 1 y))` making sense.

This gets some of them right:

```scheme
(define (fib-iter a b x y count)
  (cond ((= count 0) b)
        ((even? count)
         (fib-iter a
                   b
                   (square (fib-iter 1 0 0 1 y))
                   (+ x 1)
                   (/ count 2)))
        (else (fib-iter (+ (* b y) (* a y) (* a x))
                        (+ (* b x) (* a y))
                        x
                        y
                        (- count 1)))))
```

## Primality

### Method 1: Search for divisors

If `n` is not prime, it must have a divisor less than or equal to the square 
root of `n`.
This method tries each number between 1 and the square root of `n` to see if 
that number is a divisor of `n`.
Thus, the time complexity is O(square-root(n)).

```scheme
(define (prime? n) 
  (= n (smallest-divisor n)))

(define (smallest-divisor n) 
  (find-divisor n 2))

(define (find-divisor n test-divisor) 
  (cond ((> (square test-divisor) n) n)
        ((divides? test-divisor n) test-divisor)
        (else (find-divisor n (+ test-divisor 1)))))

(define (divides? a b) 
  (= (remainder b a) 0))
```

### Method 2: The Fermat test

#### Fermat's Little Theorem

> If `n` is a prime number and `a` is any positive integer less than `n`, then
> `a` raised to the nth power is congruent to `a` modulo `n`.

Two numbers are "congruent modulo n" if they both have the same remainder 
divided by `n`.

Fermat test:

- Given a number `n` 
- Pick a random number `a < n` 
- Compute the remainder `a^n modulo n`
- If `(a^n modulo n) != a`, n is not prime
- If it is `a`, pick another random number and test with the same method
- Confidence that `n` is prime grows with the number of tests

Compute the exponential of a number modulo another number:

```scheme
; run the fermat test a given number of times
(define (fast-prime? n times) 
  (cond ((= times 0) true) 
        ((fermat-test n) (fast-prime? n (- times 1))) 
        (else false)))

(define (fermat-test n) 
  (define (try-it a) 
    (= (expmod a n n) a)) 
  (try-it (+ 1 (random (- n 1)))))

(define (expmod base exp m) 
  (cond ((= exp 0) 1) 
        ((even? exp) 
          (remainder (square (expmod base (/ exp 2) m)) m)) 
        (else 
          (remainder (* base (expmod base (- exp 1))) m))))
```

#### Fermat's Little Theorem 2

`n` is prime when:

```
0 < a < n
a^n-1 ≡ 1 (mod n)
```

This notation means that `a^n-1 mod n = 1 mod n`.

### Miller-Rabin Test

Pick a random number `a`, where `0 < a < n`.

```
0 < a < n
a^n-1 ≡ 
```
